---
title: "plots and models"
output: 
  html_document: 
    keep_md: yes
---

Welcome to my homework 4! Like last [time](https://github.com/STAT545-UBC/joanna_bernhardt/blob/master/Homework_3/Homework_3.md), I'll be using a different dataset than Gapminder. This is a dataset that I'm working with as one of my PhD projects. 

See this [issue](https://github.com/STAT545-UBC/joanna_bernhardt/issues/7), where I ask Jenny for permission to use my data, and she approves.

The data I will be using include the nutrient content (i.e. vitamins, minerals, essential fatty acids) of several hundred species of fish :blowfish: :fish: :tropical_fish:. I've also added data on fish body size, trophic level (i.e. predator, herbivore), and latitude of where the fish was caught. 

Here's some background on the project: 

*One of the most widely studied and universally important benefits that humans derive from natural ecosystems is food provisioning. Indeed, many coastal human communities rely on wild harvests from local aquatic ecosystems to meet nutritional requirements for macronutrients, such as protein and fats, and micronutrients, such as vitamins and minerals. The value of a fish species in terms of human nutrition benefits can be quantified as the nutrient content in an edible portion relative to Recommended Daily Intake (RDI) values. The RDI is the daily intake level of a nutrient that is considered to be sufficient to meet the requirements of 97–98% of healthy individuals in every demographic. Although fisheries productivity is studied extensively, there has been surprisingly little consideration of the drivers of the nutritional quality of fisheries yields.*

Thus, the main questions I'll be adressing in this data are: 

1. What is the range of nutrient content across species? 
2. Does nutrient content vary with fish species' traits such as body size or trophic level? 

To do this I'll take a model selection approach, wherein I'll compare models which contain different species' traits as parameters to see which models fit the data best, and thus which species traits vary with nutrient content. 

I've collected nutrient and fish trait data from the peer-reviewed literature and databases such as [FishBase](https://en.wikipedia.org/wiki/FishBase). 

Loading required packages.
```{r}
library(ggplot2)
library(plotrix)
library(broom)
library(ggthemes)
suppressPackageStartupMessages(library(dplyr))
library(knitr)
suppressPackageStartupMessages(library(Hmisc))
suppressPackageStartupMessages(library(robustbase))
library(tidyr)

nut <- read.csv("~/Desktop/Nutrient_databases/nut_sept22_lwr_dec3.csv", comment.char="#", stringsAsFactors=TRUE, na.strings=c("",".","NA"))
ntbl <- tbl_df(nut)

```

Let's change variable names to more intuitive names.

```{r}
 ntbl <- ntbl %>%
  rename(species = ASFIS.Scientific.name,
         taxon = ISSCAAP_cat,
         max_length = SLMAX)
```

Pull out variables we will use in this analysis. 

```{r select variables of interest}
ntbl <- ntbl %>%
  select(species, taxon, max_length, TL, CA_mg, EPA_g, DHA_g, ZN_mg, HG_mcg, lwA, lwB, Habitat, Subgroup, Abs_lat)
```

Convert max length to max body size using length-weight conversion (W = a × L^b). For more information about this conversion approach, see this [explanation](http://www.fishbase.ca/manual/FishBaseThe_LENGTH_WEIGHT_Table.htm) on FishBase. 
```{r}
ntbl <- ntbl %>%
  mutate(max_size = (lwA * (max_length^lwB)/1000))
```

Let's clean up the df to trim out any NA values, because they were giving me some trouble. This just makes fitting the models easier. Here I'm removing any rows that have missing info for any of my variables of interest. Here I'll create two tbl_dfs which I'll call on later. One will be for calcium (a nutrient essential for bone formation and ion regulation in fish), and EPA, an omega-3 fatty acid. 
```{r}
ntbl.CA <- ntbl %>%
  filter(!is.na(max_size)) %>% 
  filter(!is.na(CA_mg)) %>% 
  filter(!is.na(taxon))

ntbl.EPA <- ntbl %>%
  filter(!is.na(Abs_lat)) %>% 
  filter(!is.na(EPA_g)) %>% 
  filter(!is.na(taxon))
```

First things, first, let's write out our models. The question I'm asking here is: Does calcium content of fish tissues vary with the body size of the fish? I.e. are smaller fish (such as sardines) better sources of calcium than large fish (such as tuna). So, I'll fit a model of calcium content as a function of body size. 

Here I'm interested in comparing the fits of the linear model using OLS and the robust fit, using robustbase. 
```{r}
size.fit.lm <- lm(log(CA_mg) ~ log(max_size), ntbl.CA)
size.fit.lmrob <- lmrob(log(CA_mg) ~ log(max_size), ntbl.CA)
```

Let's compare the fits of these two models. As you can see from the tables below, they have pretty similar fits (similar coefficient (slope) estimates and confidence intervals). Does this mean that there aren't many crazy outliers?
```{r}
lm.table <- cbind(summary(size.fit.lm)$coeff, confint(size.fit.lm))
knitr::kable(lm.table, align = 'c', format = 'markdown', digits = 4)

lmrob.table <- cbind(summary(size.fit.lmrob)$coeff, confint(size.fit.lmrob))
knitr::kable(lmrob.table, align = 'c', format = 'markdown', digits = 4)
```

Let's plot those two fits. Neither one looks all that great at this point. 
```{r}
cols <- c('lm() fit' = 'orange', 'lmrob() fit' = 'royalblue')
ntbl.CA %>% ggplot(aes(x = log(max_size), y = log(CA_mg))) + stat_summary(fun.y= "mean", geom = "point") + geom_smooth(aes(color = 'lm() fit'), method = 'lm') + geom_smooth(aes(color = 'lmrob() fit'), method = 'lmrob') +  scale_colour_manual(name="Linear Fits", values=cols) + theme(legend.position="none")
```

OK, now let's set up the initial function, in its simplest form. 
``` {r}
size.fit <- function(df) {
  (CA.fit <- lm(log(CA_mg) ~ log(max_size), df))
}
```

Here is a slightly more general function to do the same thing. More general in that here we allow the nutrient to be variable.
```{r}
size.fit2 <- function(df, nutrient) {
  model <- lm(log(nutrient) ~ log(max_size), data = df)
  y   = coef(model)[2]
  ylo = confint(model)[2]
  yhi = confint(model)[4]
  setNames(data.frame(t(c(y, ylo, yhi))), c("beta", "ylo", "yhi"))
}
```

Looks like the function works for the dataset as a whole!
```{r}
(size.fit2(ntbl.CA, ntbl.CA$CA_mg))
```

Now let's apply it by groups: taxon by taxon. 
```{r}
test<- ntbl.CA %>%  
  group_by(taxon) %>% 
  do(size.fit2(., .$CA_mg)) %>% 
  ungroup() %>% 
  arrange(desc(beta))
test  
```

Let's plot that
```{r}
test$taxon <- factor(test$taxon, levels=unique(test$taxon))
ggplot(test, aes(x=taxon, y=beta, ymin=ylo, ymax=yhi)) +
  geom_pointrange() + 
  coord_flip() + 
  geom_hline(aes(x=0), lty=2) +
  xlab('taxon') +
  ylab('Regression Coefficient') + theme(legend.position="none")

```

And now here's a more general function for fitting lms (thanks to Jenny's [post](http://stat545-ubc.github.io/block025_lm-poly.html) for this code)
```{r}
lm_general<- function(df, y, x, ...) {
  lm_formula <-
    substitute(y ~ x,
               list(y = substitute(y), x = substitute(x)))
  eval(lm(lm_formula, data = df, ...))
}

lm_general(ntbl.CA, log(max_size), log(CA_mg))

size.fits3 <- ntbl.CA %>% group_by(taxon) %>% do(tidy(lm_general(., log(max_size), log(CA_mg))))
(size.fits3)
```


same function as my initial lm function, just using robustlm this time. 
```{r}
size.fit.rob <- function(df) {
  (CA.fit <- lmrob(log(CA_mg) ~ log(max_size), df))
}
```

use the function, group by taxon
```{r}
size.fits.lm <- ntbl.CA %>%
  group_by(taxon) %>% 
  do(fit=size.fit(.))
size.fits.lm
```

Here, let's tidy the parameter estimates
```{r}
tidy.fits.lm <- size.fits.lm %>% 
  tidy(fit, conf.int = TRUE)
head(tidy.fits.lm)

augment.fits.lm <- size.fits.lm %>% augment(fit)
```

plot the residuals, by taxon
```{r}
ggplot(augment.fits.lm, aes(x= taxon, y=.resid, color = taxon)) + geom_point(size = 3) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme(legend.position="none")
```

plot the residuals, by size
```{r}
ggplot(augment.fits.lm, aes(x= log.max_size., y=.resid, color = taxon)) + geom_point(size = 3) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme(legend.position="none")
```

Let's plot those estimates! This is the most satisfying plot yet! From this plot, we can clearly see for which taxa there is a positive or negative relationship between body size and calcium content. 
```{r}  
  ggplot(subset(tidy.fits.lm, term == "log(max_size)"), aes(estimate, taxon, color = taxon)) +
    geom_point() +
    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high, height = .3)) +
    geom_vline() + theme(legend.position="none")
```

Now let's run the same models, but group by habitat. Tidy as before. 
```{r}
  size.hab <- ntbl.CA %>%
  group_by(Habitat) %>% 
  do(fit=size.fit(.))

tidy.fit.hab <- size.hab %>% 
  tidy(fit, conf.int = TRUE)

  
  ggplot(subset(tidy.fit.hab, term == "log(max_size)"), aes(estimate, Habitat, color = Habitat)) +
    geom_point() +
    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high, height = .3)) +
    geom_vline() + theme(legend.position="none")
```

Now let's try to fit with a robust approach.
```{r with robust}

#' MASS::rlm(x ~ y, data = df)$coefficients
  
  lmrob.fit <- lmrob(log(CA_mg) ~ log(max_size), ntbl)
  lm.fit <- lm(log(CA_mg) ~ log(max_size), ntbl)
  rlm.fit <- MASS::rlm(log(CA_mg) ~ log(max_size), ntbl.CA, model = TRUE, x.ret = TRUE, y.ret = FALSE)
 
```
Let's make a function to compare the robust and regular lm fits. Note to self: add AIC here.
```{r}
suppressPackageStartupMessages(require(MASS))
compare_models <- function(df, nutrient) {
  model_lm <- lm(log(nutrient) ~ log(max_size), data = df)
  lm_df <- data.frame(slope = coef(model_lm)[2], intercept = coef(model_lm)[1], model = "normal")
  model_rlm <- rlm(log(nutrient) ~ log(max_size), data = df, method = "MM")
  rlm_df <- data.frame(slope = coef(model_rlm)[2], intercept = coef(model_rlm)[1], model = "robust")
  return(rbind(lm_df, rlm_df))
}

knitr::kable(compare_models(ntbl.CA, ntbl.CA$CA_mg), format = "markdown")

```

OK, now let's apply this function across all taxa. OK this doesn't work...maybe because the different fitting approaches drop different numbers of taxa??
```{r}
#'model.comparison <- ntbl.CA %>% group_by(taxon) %>% do(model.comp = compare_models(., ntbl.CA$CA_mg))
```


Function for finding the max residuals. This function allows us to figure out which taxon has the highest residual values...an indication of worst fit. 
```{r}
mean_resid <- function(df) {
    size.fit <- lm(log(CA_mg) ~ log(max_size), df)
    x <- mean(abs(resid(size.fit)))
    y <- setNames(data.frame(t(x)), c("mean_residual"))
    y
}

mean_resid(ntbl.CA %>% filter(Habitat == "marine"))


resid_1 <- ntbl.CA %>%
group_by(taxon) %>% 
do(mean_resid(.)) %>% 
  unnest(mean_residual) %>% 
  arrange(desc(mean_residual))
(resid_1) #' here we see that the group 'Miscellaneous freshwater fishes' has the highest residuals, i.e. worst fit to the linear model, which does make sense since it's the most 'grab bag' of the groups, being 'miscellaneous' and all. 

```

New function idea! Let's find the % of each taxon that has EPA values above RDI

So, for one taxon, what would this look like?
```{r}
#' Let's try for one taxon
EPA.RDI <- ntbl.EPA %>%
  filter(taxon == "Salmons, trouts, smelts") %>% 
  mutate(RDI = ifelse(EPA_g > 0.25, 1, 0)) %>% 
  group_by(species) %>% 
 mutate(per.RDI = sum(RDI)/n_distinct(EPA_g)) %>% 
  mutate(mean.per.RDI= mean(per.RDI))    

#' Here's the function
epa.prop <- function(df) {
  (EPA.RDI <- df %>%
  mutate(RDI = ifelse(EPA_g > 0.25, 1, 0)) %>% 
  group_by(species) %>% 
 mutate(per.RDI = sum(RDI)/n_distinct(EPA_g)) %>% 
  mutate(mean.per.RDI= mean(per.RDI))) 
}

#' Here it is applied to my dataset, grouped by taxon, and unnested, summarised etc. The final output that I want is the percentage of fish species in each taxon that reaches a threshold of 25% of RDI in one portion.
epa.prp <- ntbl.EPA %>%
  do(EPA.prp=epa.prop(.)) %>% 
    unnest(EPA.prp) %>%
  group_by(taxon) %>% 
  summarise(meanRDI = mean(mean.per.RDI)) %>% 
  mutate(mean.percent.RDI = meanRDI * 100) %>% 
  arrange(mean.percent.RDI)

#' And this graph shows these percentages ordered by increasing percentage. Yahoo! Success!
ggplot(epa.prp, aes(x = reorder(taxon, mean.percent.RDI), y = mean.percent.RDI, color = taxon)) + geom_point(size = 6) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme(legend.position="none")

head(epa.prp)
```       
    
 Here's let's look at latitude
```{r}
epa.prp2 <- ntbl.EPA %>%
  do(EPA.prp=epa.prop(.)) %>% 
    unnest(EPA.prp) %>%
arrange(desc(Abs_lat))


#' this figure shows the percentage of each taxon that reaches RDI, as arranged by increasing latitude
ggplot(epa.prp2, aes(x = reorder(taxon, Abs_lat), y = mean.per.RDI, color = taxon)) + stat_summary(fun.y= "mean", geom = "point") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme(legend.position="none")


p <- ggplot(subset(ntbl.EPA, Habitat == "marine"), aes(x=Abs_lat, y=log(EPA_g)))
p + stat_summary(aes(y = log(EPA_g)), fun.y=mean, geom = "point") + geom_hline(aes(yintercept=log(0.5))) + stat_smooth(method = "lm") + theme_pander() + xlab("Absolute latitude") + ylab("log EPA content, g/100g portion") + theme(legend.position="none")

```
   
    
         
this doesn't work, because I can't figure out how to pull out the right column to unnest.
epa.prop2 <- function(df) {
  (EPA.RDI <- df %>%
  mutate(RDI = ifelse(EPA_g > 0.25, 1, 0)) %>% 
  group_by(species) %>% 
 mutate(per.RDI = sum(RDI)/n_distinct(EPA_g)) %>% 
  mutate(mean.per.RDI= mean(per.RDI))) %>%  
  unnest(.[,2]) %>% ##this is where I run into problems. 
  group_by(taxon) %>% 
  summarise(meanRDI = mean(mean.per.RDI)) %>% 
  mutate(mean.percent.RDI = meanRDI * 100)
}
epa.prop2(ntbl.EPA)
head(epa.prp)


```{r}
library(MuMIn)
EPA.1 <- lm(log(EPA_g) ~ log(max_size)*TL + log(max_size)*Abs_lat + log(max_size)*Habitat, data=ntbl.EPA)
EPA.2 <- lm(log(EPA_g) ~ log(max_size)*Abs_lat + log(max_size)*Habitat, data=ntbl.EPA)

model.sel(EPA.1, EPA.2)

#' this function allows me to average the top models and then prints out a table with the relevant AIC values, and importance weights. One thing I would like it to be able to do, but haven't yet figured out, is to select the top model set and then average them, all in one function. Right now I need to do the model selection process outside the function and plop in the top models into the function. It'd be great if I could automate that process somehow!
AIC.table <- function(mod1, mod2) {
  model.average <-  model.avg(mod1, mod2)
  return((msTable <- model.average$msTable))
  }

(AIC.table(EPA.1, EPA.2))
```


